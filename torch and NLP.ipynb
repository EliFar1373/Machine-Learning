{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23d9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization: Splitting text into individual words or tokens.\n",
    "#Lowercasing: Converting all text to lowercase to treat words like \"Apple\" and \"apple\" the same.\n",
    "#Removing Stop Words: Words like \"the\", \"and\", \"is\", etc., that donâ€™t add much meaning.\n",
    "#Removing Punctuation: We often remove punctuation as it's generally not needed for analysis.\n",
    "#Stemming/Lemmatization: Reducing words to their root form (e.g., \"running\" -> \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f8e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead9fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea7fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Sample text\n",
    "text = \"This is an example sentence, showcasing NLP preprocessing!\"\n",
    "doc=nlp(text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77b9d84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'showcasing',\n",
       " 'nlp',\n",
       " 'preprocessing',\n",
       " '!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=[token.text for token in doc if token.text]\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "803b9d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'sentence', ',', 'showcasing', 'nlp', 'preprocessing', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=[token.text for token in doc if token.text not in stopwords.words('english')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e50845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'sentence', 'showcasing', 'nlp', 'preprocessing']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=[token.text for token in doc if token.text not in stopwords.words('english') and token.text not in string.punctuation]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0eb596d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example' 'nlp' 'preprocessing' 'sentence' 'showcasing']\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#converts the list of text documents into a matrix of TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "\n",
    "x=vectorizer.fit_transform(tokens)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(x.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52644968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa810e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'programming']\n",
      "['hate', 'bugs']\n",
      "['coding', 'fun']\n",
      "['debugging', 'annoying']\n",
      "['love programming', 'hate bugs', 'coding fun', 'debugging annoying']\n"
     ]
    }
   ],
   "source": [
    "documents = [\"I love programming\", \"I hate bugs\", \"Coding is fun\", \"Debugging is annoying\"]\n",
    "labels = [1, 0, 1, 0]  # Labels indicating positive/negative sentiment\n",
    "# Sample labeled data (labels: 1 = positive, 0 = negative)\n",
    "\n",
    "\n",
    "doc3=[]\n",
    "for doc in documents:\n",
    "    doc1=nlp(doc.lower())\n",
    "    doc2=[token.text for token in doc1 if token.text not in stopwords.words('english') and token.text not in string.punctuation]\n",
    "    print(doc2)\n",
    "    doc3.append(\" \".join(doc2))\n",
    "print(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65eadb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aacuracy:0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "\n",
    "\n",
    "x=vectorizer.fit_transform(doc3)\n",
    "y=labels\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "classifier=LogisticRegression()\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "y_pred=classifier.predict(x_test)\n",
    "print(f'aacuracy:{accuracy_score(y_test,y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6a2ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6],\n",
      "        [2, 8]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "array=np.array([[5,6],[2,8]])\n",
    "x=torch.tensor(array)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ea217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43d6e86d",
   "metadata": {},
   "source": [
    "# define a neural network using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb47a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network\n",
    "class Neural(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(Neural,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size,hidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc2=nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898843f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afbbffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6708,  0.8500]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "output=model.forward(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab5e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5175683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Neural(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cab6d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,loss:0.12254390120506287\n",
      "epoch 10,loss:0.09279361367225647\n",
      "epoch 20,loss:0.0709526315331459\n",
      "epoch 30,loss:0.055224377661943436\n",
      "epoch 40,loss:0.04389578476548195\n",
      "epoch 50,loss:0.03563286364078522\n",
      "epoch 60,loss:0.02949199639260769\n",
      "epoch 70,loss:0.024830393493175507\n",
      "epoch 80,loss:0.021216748282313347\n",
      "epoch 90,loss:0.018360242247581482\n"
     ]
    }
   ],
   "source": [
    "# Sample training data (Batch size = 2)\n",
    "X_train = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # Features\n",
    "y_train = torch.tensor([0, 1])  # Class labels (NOT one-hot encoded!)\n",
    "\n",
    "\n",
    "# Sample training loop\n",
    "epochs=100 # Number of training iterations\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() # Reset gradients\n",
    "    output=model(X_train)  # Forward pass\n",
    "    loss=criterion(output,y_train) # Compute loss\n",
    "    loss.backward() # Backpropagation \n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print(f'epoch {epoch},loss:{loss.item()}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da79450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(modelm,'')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29960214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, r'C:\\Users\\elham\\Desktop\\excel2\\model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c48639f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict, save parametrs and weight only without arthitecture of model\n",
    "torch.save(model.state_dict(), r'C:\\Users\\elham\\Desktop\\excel2\\model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7721544b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neural(\n",
       "  (fc1): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model's state_dict\n",
    "model = Neural(input_size=3, hidden_size=5, output_size=2)  # Define the model architecture\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\elham\\Desktop\\excel2\\model2'))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b16c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da1078ff",
   "metadata": {},
   "source": [
    "# Define CNN using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b50a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Nuralcon(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nuralcon,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5,padding=2)\n",
    "        self.Relu1=nn.ReLU()\n",
    "        self.pool1=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout=nn.Dropout(0.25)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(32 * 7 * 7,123)\n",
    "        self.fc2=nn.Linear(123,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.pool1(self.Relu1(self.conv1(x)))\n",
    "        x=self.pool2(self.relu2(self.conv2(x)))\n",
    "        x=self.dropout(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model2=Nuralcon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "its same as this one\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(filters=16,kernel_size=(5,5),padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(123,activation='relu'))\n",
    "model.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
