{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04602f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset from TensorFlow\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Flatten 28x28 images to a vector\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')  # 10 classes for digits 0-9\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf9db1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#import \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load dataset using torchvision\n",
    "transform=transforms.compose(transforms.ToString(),transforms.Normalize(0.5,0.5))\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "\n",
    "# Define a simple neural network\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2,self).__init__()\n",
    "        \n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(28*28,128)\n",
    "        self.relu=nn.Relu()\n",
    "        self.fc2=nn.Linear(128,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.flatten(x)\n",
    "        x=self.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        \n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model=Model2()\n",
    "critirion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.adam(model.paramerters(),lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epochs in range(5):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output=model(images)\n",
    "        loss=critirion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epochs{epoch+1},loss:{loss.item():.4f}\")\n",
    "   \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1bec7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\http\\client.py\", line 463, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\http\\client.py\", line 507, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 173, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 203, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 315, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 472, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 341, in resolve\n",
      "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 140, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 128, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 32, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 204, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 295, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 305, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 550, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 239, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 145, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 144, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\Users\\elham\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)\", ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84f51bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.19.6\n",
      "Uninstalling protobuf-3.19.6:\n",
      "  Successfully uninstalled protobuf-3.19.6\n",
      "Found existing installation: tensorflow-intel 2.11.1\n",
      "Uninstalling tensorflow-intel-2.11.1:\n",
      "  Successfully uninstalled tensorflow-intel-2.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\elham\\anaconda3\\lib\\site-packages (21.2.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\elham\\anaconda3\\lib\\site-packages (61.2.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\elham\\anaconda3\\lib\\site-packages (0.37.1)\n",
      "Collecting wheel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178B9E1D00>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pip/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178B9E1A30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pip/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178B9E1490>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pip/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178B9E9610>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pip/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178B9E9460>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pip/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA109A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/setuptools/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA10580>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/setuptools/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA105B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/setuptools/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA104C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/setuptools/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA10D30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/setuptools/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA1A880>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA1AC70>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/wheel/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BA1ACA0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/wheel/\n",
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BAEC550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BAEC730>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BAEC910>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BAECAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BAECCD0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl\n",
      "ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl (Caused by NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002178BAECEB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf tensorflow-intel -y\n",
    "!pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbdf1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.19.1\n",
      "Uninstalling protobuf-3.19.1:\n",
      "  Successfully uninstalled protobuf-3.19.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Skipping tensorflow-intel as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf tensorflow-intel -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f507e151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\elham\\anaconda3\\lib\\site-packages (2.11.1)\n",
      "Collecting tensorflow-intel==2.11.1\n",
      "  Downloading tensorflow_intel-2.11.1-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (1.22.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.1->tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.1->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (0.4.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.23.2 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\elham\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (2.23.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.1->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\elham\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.1->tensorflow) (3.0.4)\n",
      "Installing collected packages: protobuf, tensorflow-intel\n",
      "Successfully installed protobuf-3.19.6 tensorflow-intel-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define the model using `.add()`\n",
    "model = keras.Sequential()  # Initialize model\n",
    "\n",
    "# Add layers one by one\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))  # Flatten image to 1D\n",
    "model.add(keras.layers.Dense(128, activation='relu'))  # Hidden layer\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))  # Output layer (10 classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa167206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transformations (convert images to tensors & normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download and load the training dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the testing dataset\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1) # Conv layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)  # Second Conv layer\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)  # Flatten the image\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNN()\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5  # Number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # A single grayscale image (batch_size=1)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Should print torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reshape images for TensorFlow CNN (add channel dimension)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Split the dataset using sklearn\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Create a Sequential CNN Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')  # 10 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_data=(X_val, Y_val))\n",
    "\n",
    "\n",
    "# Get predictions on test data\n",
    "y_pred_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert softmax output to class labels\n",
    "\n",
    "# Evaluate accuracy using sklearn\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample dataset (Study hours vs Pass/Fail)\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0: Fail, 1: Pass\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for a student who studies 6.5 hours\n",
    "prob = model.predict_proba([[6.5]])\n",
    "print(\"Probability of passing:\", prob[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a61634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate dummy data (House size vs Price)\n",
    "X = np.array([500, 600, 700, 800, 900, 1000, 1100]).reshape(-1, 1)  # House size\n",
    "y = np.array([150, 180, 210, 240, 270, 300, 330])  # Price in thousands\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
    "plt.plot(X_test, y_pred, color='red', linestyle=\"dashed\", label=\"Predicted Line\")\n",
    "plt.xlabel(\"House Size (sq ft)\")\n",
    "plt.ylabel(\"Price (in $1000)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12664685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[25, 50000], [30, 60000], [35, 70000], [40, 80000], [45, 90000], [50, 100000]])\n",
    "y = np.array([0, 0, 1, 1, 1, 1])  # 0: No, 1: Yes (Buys Car)\n",
    "\n",
    "# Train Decision Tree model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for a 32-year-old with a salary of 65,000\n",
    "prediction = model.predict([[32, 65000]])\n",
    "print(\"Will they buy the car?\", \"Yes\" if prediction[0] == 1 else \"No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (Iris flowers)\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, :2], iris.target  # Use only 2 features for visualization\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74748d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71aa8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN model (k=3 neighbors)\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd87034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ef06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, mean_absolute_error, \n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Sample Data for Classification (e.g., Logistic Regression, SVM, etc.)\n",
    "# y_true and y_pred are actual and predicted labels, respectively.\n",
    "y_true_class = np.array([0, 1, 1, 0, 1, 0, 1, 0, 1, 0])  # Actual labels for classification\n",
    "y_pred_class = np.array([0, 1, 1, 1, 1, 0, 0, 0, 1, 0])  # Predicted labels for classification\n",
    "\n",
    "# Sample Data for Regression (e.g., Linear Regression, Decision Tree Regressor, etc.)\n",
    "# y_true_reg and y_pred_reg are actual and predicted values, respectively.\n",
    "y_true_reg = np.array([150, 200, 250, 300, 350, 400, 450, 500, 550, 600])  # Actual values for regression\n",
    "y_pred_reg = np.array([148, 198, 252, 305, 347, 399, 460, 505, 540, 595])  # Predicted values for regression\n",
    "\n",
    "# --- Classification Evaluation ---\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "print(\"Classification Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true_class, y_pred_class)\n",
    "print(\"Classification Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true_class, y_pred_class)\n",
    "print(\"Classification Recall:\", recall)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true_class, y_pred_class)\n",
    "print(\"Classification F1 Score:\", f1)\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_true_class, y_pred_class)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true_class, y_pred_class)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# --- Regression Evaluation ---\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "print(\"Regression Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "print(\"Regression Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Regression Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# R-squared (R)\n",
    "r2 = r2_score(y_true_reg, y_pred_reg)\n",
    "print(\"Regression R Score:\", r2)\n",
    "\n",
    "# --- Cross-Validation (for both Classification and Regression) ---\n",
    "\n",
    "# You can use cross-validation on a classification or regression model.\n",
    "# For demonstration, well use Logistic Regression for classification, and Linear Regression for regression.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.datasets import load_iris, make_regression\n",
    "\n",
    "# Classification: Cross-validation on Logistic Regression\n",
    "X_class = load_iris().data  # Iris dataset features\n",
    "y_class = load_iris().target  # Iris dataset target labels\n",
    "classification_model = LogisticRegression(max_iter=200)\n",
    "\n",
    "classification_cv_scores = cross_val_score(classification_model, X_class, y_class, cv=5, scoring='accuracy')\n",
    "print(\"\\nClassification Cross-Validation Scores:\", classification_cv_scores)\n",
    "print(\"Mean Classification CV Accuracy:\", classification_cv_scores.mean())\n",
    "\n",
    "# Regression: Cross-validation on Linear Regression\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)  # Example regression data\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "regression_cv_scores = cross_val_score(regression_model, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"\\nRegression Cross-Validation Scores (Negative MSE):\", regression_cv_scores)\n",
    "print(\"Mean Regression CV MSE:\", -regression_cv_scores.mean())  # Negate because it's returned as negative MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0edecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)  # Removes rows with missing values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # You can use 'median' or 'most_frequent'\n",
    "data['column_name'] = imputer.fit_transform(data[['column_name']])\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputer (Transforms DataFrame into NumPy array, so we reassign)\n",
    "data[['Age', 'Salary']] = imputer.fit_transform(data[['Age', 'Salary']])\n",
    "\n",
    "\n",
    "# Fill missing values with column mean\n",
    "data['Age'] = data['Age'].fillna(data['Age'].mean()) \n",
    "data['Salary'] = data['Salary'].fillna(data['Salary'].mean())\n",
    "\n",
    "df_filled = data.fillna(value=0)  # Creates a new DataFrame with NaN replaced by 0\n",
    "data.fillna(value=0, inplace=True)  # Directly modifies `data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical Variables\n",
    "#1:One-Hot Encoding:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(data[['column_name']])\n",
    "\n",
    "data = pd.get_dummies(data, columns=['column_name'])\n",
    "\n",
    "#Scikit-learn functions require 2D input, so using [['column_name']]\n",
    "\n",
    "\n",
    "#2:Label Encoding:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data['column_name'] = encoder.fit_transform(data['column_name'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature scaling \n",
    "#Normally distributed data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "#Centers the data around mean = 0 and standard deviation = 1.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "#Scales the data to a specific range (usually 0 to 1)\n",
    "\n",
    "\n",
    "#\tData with outliers\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# Apply Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "df['Scaled_Value'] = scaler.fit_transform(df[['Value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4664f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers can skew results, especially for models that rely on distances (e.g., KNN, SVM) or models sensitive to data distribution (e.g., Linear Regression).\n",
    "Common Approaches:\n",
    "#Visualize data:\n",
    "\n",
    "#Use boxplots or scatterplots to visually detect outliers.\n",
    "#Statistical Methods:\n",
    "\n",
    "# standard deviations or interquartile ranges (IQR).\n",
    "Q1 = data['column'].quantile(0.25)\n",
    "Q3 = data['column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data_clean = data[(data['column'] >= (Q1 - 1.5 * IQR)) & (data['column'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "IQR = q3 - q1\n",
    "lower_bound = q1 - 1.5 * IQR\n",
    "upper_bound = q3 + 1.5 * IQR\n",
    "\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n",
    "\n",
    " #Interquartile Range (IQR) method,Works well for skewed data\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " #secodn :Best for normally distributed data, Fails if data is not normally distribu\n",
    "\n",
    " # Calculate Mean and Standard Deviation\n",
    "mean = df['Value'].mean()\n",
    "std_dev = df['Value'].std()\n",
    "\n",
    "# Define outlier thresholds\n",
    "lower_bound = mean - 3 * std_dev\n",
    "upper_bound = mean + 3 * std_dev\n",
    "\n",
    "# Identify outliers\n",
    "outliers = df[(df['Value'] < lower_bound) | (df['Value'] > upper_bound)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Split the data into training and testing sets (commonly 70%-30% or 80%-20% split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23437bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "#Wrapper Methods:\n",
    "#Use algorithms like Recursive Feature Elimination (RFE) to select important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter Methods\n",
    "#1Correlation-based Selection\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "#CRIM: is for correlation between target and feature\n",
    "# Select features that have a high correlation with the target variable\n",
    "cor_target = abs(correlation_matrix[\"CRIM\"])  # Let's say we're interested in \"CRIM\"\n",
    "relevant_features = cor_target[cor_target > 0.5]  # Selecting features with correlation > 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #2Chi-Square Test\n",
    "#Explanation:\n",
    "#The chi-square test is used for categorical data. It tests whether there is a significant relationship between each feature and the target variable. Features with low chi-square values are typically dropped.\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Apply the chi-square test\n",
    "chi2_selector = SelectKBest(chi2, k=2)  # Selecting top 2 features based on chi-square score\n",
    "X_new = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[chi2_selector.get_support()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3Decision Trees / Random Forest Feature Importance\n",
    "#Explanation:\n",
    "#Tree-based models like Random Forests can automatically compute feature \n",
    "#importance based on how much each feature contributes to reducing the impurity (e.g., Gini or entropy) in the decision tree.\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Sort features based on importance\n",
    "sorted_indices = importances.argsort()\n",
    "print(\"Selected features:\", X.columns[sorted_indices[::-1]])  # Sorted from most to least important\n",
    " \n",
    "# 4: Dimensionality Reduction (PCA)\n",
    "#PCA is a dimensionality reduction technique that transforms the data into a new coordinate system,transfer to 2 dimensional\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "\n",
    "\n",
    "#5:\n",
    "#utual Information (MI) measures the dependency between two variables (in this case, the features and the target variable). It quantifies how much information the target variable\n",
    "#provides about each feature. Higher values of MI mean stronger relationships between the feature and the target.\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = mutual_info_classif(X, y)\n",
    "selected_features = X.columns[mi > 0.1]\n",
    "\n",
    "\n",
    "\n",
    "#6:\n",
    "#rom sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Apply Lasso regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Print selected features (non-zero coefficients)\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#7:\n",
    "# Wrapper Methods\n",
    "#a. Recursive Feature Elimination (RFE)\n",
    "#Explanation:\n",
    "#RFE recursively removes the least important features based on a chosen model. It uses the model to assess feature importance and eliminates features with the least contribution.\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Apply RFE (select top 2 features)\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "# Print selected features\n",
    "selected_features = X.columns[rfe.support_]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
