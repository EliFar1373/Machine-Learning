{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d92fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter Methods\n",
    "#1Correlation-based Selection\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "#CRIM: is for correlation between target and feature\n",
    "# Select features that have a high correlation with the target variable\n",
    "cor_target = abs(correlation_matrix[\"CRIM\"])  # Let's say we're interested in \"CRIM\"\n",
    "relevant_features = cor_target[cor_target > 0.5]  # Selecting features with correlation > 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #2Chi-Square Test\n",
    "#Explanation:\n",
    "#The chi-square test is used for categorical data. It tests whether there is a significant relationship between each feature and the target variable. Features with low chi-square values are typically dropped.\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Apply the chi-square test\n",
    "chi2_selector = SelectKBest(chi2, k=2)  # Selecting top 2 features based on chi-square score\n",
    "X_new = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[chi2_selector.get_support()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3Decision Trees / Random Forest Feature Importance\n",
    "#Explanation:\n",
    "#Tree-based models like Random Forests can automatically compute feature \n",
    "#importance based on how much each feature contributes to reducing the impurity (e.g., Gini or entropy) in the decision tree.\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importance\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Sort features based on importance\n",
    "sorted_indices = importances.argsort()\n",
    "print(\"Selected features:\", X.columns[sorted_indices[::-1]])  # Sorted from most to least important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856b353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Dimensionality Reduction (PCA)\n",
    "#PCA is a dimensionality reduction technique that transforms the data into a new coordinate system,transfer to 2 dimensional\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "\n",
    "\n",
    "#5:\n",
    "#utual Information (MI) measures the dependency between two variables (in this case, the features and the target variable). It quantifies how much information the target variable\n",
    "#provides about each feature. Higher values of MI mean stronger relationships between the feature and the target.\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = mutual_info_classif(X, y)\n",
    "selected_features = X.columns[mi > 0.1]\n",
    "\n",
    "\n",
    "\n",
    "#6:\n",
    "#rom sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Apply Lasso regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Print selected features (non-zero coefficients)\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#7:\n",
    "# Wrapper Methods\n",
    "#a. Recursive Feature Elimination (RFE)\n",
    "#Explanation:\n",
    "#RFE recursively removes the least important features based on a chosen model. It uses the model to assess feature importance and eliminates features with the least contribution.\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Apply RFE (select top 2 features)\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "# Print selected features\n",
    "selected_features = X.columns[rfe.support_]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
